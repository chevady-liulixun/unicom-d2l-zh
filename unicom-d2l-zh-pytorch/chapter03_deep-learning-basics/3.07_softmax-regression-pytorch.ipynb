{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7 softmax回归的简洁实现\n",
    "\n",
    "我们在3.3节“[线性回归的简洁实现](3.03_linear-regression-pytorch.ipynb)”中已经了解了使用Pytorch实现模型的便利。下面，让我们再次使用Pytorch来实现一个softmax回归模型。首先导入所需的包或模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T08:42:25.752763Z",
     "start_time": "2019-10-16T08:42:22.156695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7.1 获取和读取数据\n",
    "\n",
    "我们仍然使用Fashion-MNIST数据集和上一节中设置的批量大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T08:42:27.192023Z",
     "start_time": "2019-10-16T08:42:27.016914Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7.2 定义和初始化模型\n",
    "\n",
    "在3.4节（[softmax回归](3.04_softmax-regression.ipynb)）中提到，softmax回归的输出层是一个全连接层，所以我们用一个线性模块就可以了。因为前面我们数据返回的每个batch样本`x`的形状为(batch_size, 1, 28, 28), 所以我们要先用`view()`将`x`的形状转换成(batch_size, 784)才送入全连接层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T08:43:04.730564Z",
     "start_time": "2019-10-16T08:43:04.693070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): FlattenLayer()\n",
      "  (1): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "    def forward(self, x): # x shape: (batch, *, *, ...)\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "# 方法一：直接把输入层和输出层插入到模型神经网络里，以数字为索引\n",
    "net =  nn.Sequential(\n",
    "        FlattenLayer(),\n",
    "        nn.Linear(num_inputs, num_outputs))\n",
    "\n",
    "print(net)\n",
    "\n",
    "# 方法二：使用OrderedDict，建立自己的索引\n",
    "# from collections import OrderedDict\n",
    "# net = nn.Sequential(\n",
    "#        OrderedDict([\n",
    "#          ('flatten', FlattenLayer()),\n",
    "#          ('linear', nn.Linear(num_inputs, num_outputs))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T08:43:18.090750Z",
     "start_time": "2019-10-16T08:43:18.043887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#方法一：\n",
    "init.normal_(net[1].weight, mean=0, std=0.01)\n",
    "init.constant_(net[1].bias, val=0) \n",
    "\n",
    "#方法二：\n",
    "# init.normal_(net.linear.weight, mean=0, std=0.01)\n",
    "# init.constant_(net.linear.bias, val=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7.3 softmax和交叉熵损失函数\n",
    "\n",
    "如果做了上一节的练习，那么你可能意识到了分开定义softmax运算和交叉熵损失函数可能会造成数值不稳定。因此，PyTorch提供了一个包括softmax运算和交叉熵损失计算的函数。它的数值稳定性更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T08:43:42.717095Z",
     "start_time": "2019-10-16T08:43:42.701235Z"
    }
   },
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7.4 定义优化算法\n",
    "\n",
    "我们使用学习率为0.1的小批量随机梯度下降作为优化算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T08:43:53.218575Z",
     "start_time": "2019-10-16T08:43:53.180089Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7.5 训练模型\n",
    "\n",
    "接下来，我们使用上一节中定义的训练函数来训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T08:44:24.427523Z",
     "start_time": "2019-10-16T08:44:24.392277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T08:44:30.184505Z",
     "start_time": "2019-10-16T08:44:30.127641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(list(net.parameters())[0].device)\n",
    "net.to(device)\n",
    "print(list(net.parameters())[0].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T08:46:27.550029Z",
     "start_time": "2019-10-16T08:44:49.977808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu\n",
      "epoch 1, loss 0.0031, train acc 0.748, test acc 0.780\n",
      "epoch 2, loss 0.0022, train acc 0.813, test acc 0.806\n",
      "epoch 3, loss 0.0021, train acc 0.824, test acc 0.814\n",
      "epoch 4, loss 0.0020, train acc 0.832, test acc 0.824\n",
      "epoch 5, loss 0.0019, train acc 0.837, test acc 0.783\n",
      "97.54097652435303\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "from time import time\n",
    "start = time()\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* PyTorch提供的函数往往具有更好的数值稳定性。\n",
    "* 可以使用PyTorch更简洁地实现softmax回归。\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 尝试调一调超参数，如批量大小、迭代周期和学习率，看看结果会怎样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
