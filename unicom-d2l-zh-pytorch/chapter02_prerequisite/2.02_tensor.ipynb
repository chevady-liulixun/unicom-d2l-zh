{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 数据操作\n",
    "\n",
    "在深度学习中，我们通常会频繁地对数据进行操作。作为动手学深度学习的基础，本节将介绍如何对内存中的数据进行操作。\n",
    "\n",
    "在PyTorch中，`Tensor`是一个类，也是存储和变换数据的主要工具。为了简洁，本书常将`Tensor`实例直接称作`Tensor`。如果你之前用过NumPy，你会发现`Tensor`和NumPy的多维数组非常类似。然而，`Tensor`提供GPU计算和自动求梯度等更多功能，这些使`Tensor`更加适合深度学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 创建`Tensor`\n",
    "\n",
    "我们先介绍`Tensor`的最基本功能。如果对这里用到的数学操作不是很熟悉，可以参阅附录中[“数学基础”](../chapter11_appendix/11.02_math.ipynb)一节。\n",
    "\n",
    "首先从PyTorch导入`torch`模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:13:12.992020Z",
     "start_time": "2019-11-21T19:13:11.426846Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0) # 生成随机数用的种子\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">㊟ [`torch.manual_seed()`](https://pytorch.org/docs/stable/torch.html#torch._C.Generator.manual_seed)的作用是设置用于生成随机数的种子。\n",
    "\n",
    ">㊟ [`torch.cuda.manual_seed()`](https://pytorch.org/docs/stable/cuda.html#torch.cuda.manual_seed)作用与`torch.manual_seed()`相同，只不过是在当前GPU上执行，如果不可执行，比如根本就没有GPU，那么它也不报错。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们用`torch.arange`函数创建一个行向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:13:17.740943Z",
     "start_time": "2019-11-21T19:13:17.664274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时返回了一个`tensor`实例，其中包含了从0开始的12个连续整数。从打印`x`时显示的结果可以看出，这个`tensor`是长度为12的一维数组。\n",
    "\n",
    "我们可以通过`shape`属性来获取`tensor`实例的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:13:20.323947Z",
     "start_time": "2019-11-21T19:13:20.308255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面使用`torch.reshape`函数把行向量`x`的形状改为(3, 4)，也就是一个3行4列的矩阵，并记作`X`。除了形状改变之外，`X`中的元素保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:13:22.556438Z",
     "start_time": "2019-11-21T19:13:22.527835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.reshape(x, (3, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ㊟ 常用改变tensor形状的函数是[`torch.Tensor.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view),用法如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:13:26.404065Z",
     "start_time": "2019-11-21T19:13:26.374706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = x.view(3, 4)\n",
    "X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意`X`属性中的形状发生了变化。上面`x.reshape((3, 4))`也可写成`x.reshape((-1, 4))`或`x.reshape((3, -1))`。由于`x`的元素个数是已知的，这里的`-1`是能够通过元素个数和其他维度的大小推断出来的。\n",
    "\n",
    "接下来，我们创建一个各元素为0，形状为(2, 3, 4)的张量。实际上，之前创建的向量和矩阵都是特殊的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:13:32.970056Z",
     "start_time": "2019-11-21T19:13:32.944791Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似地，我们可以创建各元素为1的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:13:40.455068Z",
     "start_time": "2019-11-21T19:13:40.397319Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以通过Python的列表（list）指定需要创建的`NDArray`中每个元素的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:13:44.736977Z",
     "start_time": "2019-11-21T19:13:44.712235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 4, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [4, 3, 2, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有些情况下，我们需要随机生成`Tensor`中每个元素的值。下面我们创建一个形状为(3, 4)的`Tensor`。它的每个元素都随机采样于均值为0、标准差为1的正态分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T19:13:46.853923Z",
     "start_time": "2019-11-21T19:13:46.788966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n",
       "        [-1.0845, -1.3986,  0.4033,  0.8380],\n",
       "        [-0.7193, -0.4033, -0.5966,  0.1820]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(0, 1, size=(3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ㊟ [`torch.normal()`](https://pytorch.org/docs/stable/torch.html#torch.normal)和[`torch.randn()`](https://pytorch.org/docs/stable/torch.html#torch.randn)都可以返回一个正态分布的随机数组成的`tensor`。`torch.randn()`的功能就是返回一个标准的正态分布(均值为$0$,标准差为$1$)随机数`tensor`,用法如下例:☟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T08:38:54.227140Z",
     "start_time": "2019-11-19T08:38:54.195893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5091,  2.0820,  1.7067,  2.3804],\n",
       "        [-1.1256, -0.3170, -1.0925, -0.0852],\n",
       "        [ 0.3276, -0.7607, -1.5991,  0.0185],\n",
       "        [-0.7504,  0.1854,  0.6211,  0.6382]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 运算\n",
    "\n",
    "`torch.Tensor`支持大量的运算符（operator）。例如，我们可以对之前创建的两个形状为(3, 4)的`Tensor`做按元素加法。所得结果形状不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T08:39:10.987871Z",
     "start_time": "2019-11-19T08:39:10.956624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  2,  6,  6],\n",
       "        [ 5,  7,  9, 11],\n",
       "        [12, 12, 12, 12]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X + Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按元素乘法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T08:39:14.849658Z",
     "start_time": "2019-11-19T08:39:14.824427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  8,  9],\n",
       "        [ 4, 10, 18, 28],\n",
       "        [32, 27, 20, 11]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X * Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按元素除法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T08:39:20.589782Z",
     "start_time": "2019-11-19T08:39:20.558539Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  0,  1],\n",
       "        [ 4,  2,  2,  1],\n",
       "        [ 2,  3,  5, 11]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X / Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按元素做指数运算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T08:39:29.081615Z",
     "start_time": "2019-11-19T08:39:29.003506Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.3891,  2.7183, 54.5982, 20.0855],\n",
       "        [ 2.7183,  7.3891, 20.0855, 54.5982],\n",
       "        [54.5982, 20.0855,  7.3891,  2.7183]], dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.to(dtype=torch.float64).exp() #得先把Y转换成浮点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了按元素计算外，我们还可以使用[`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm)函数做矩阵乘法。下面将`X`与`Y`的转置做矩阵乘法。由于`X`是3行4列的矩阵，`Y`转置为4行3列的矩阵，因此两个矩阵相乘得到3行3列的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T08:40:05.322653Z",
     "start_time": "2019-11-19T08:40:05.270383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[2, 1, 4, 3],\n",
      "        [1, 2, 3, 4],\n",
      "        [4, 3, 2, 1]])\n",
      "tensor([[ 18,  20,  10],\n",
      "        [ 58,  60,  50],\n",
      "        [ 98, 100,  90]])\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(Y)\n",
    "print(torch.mm(X, Y.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以将多个`Tensor`连结（concatenate）。下面分别在行上（维度0，即形状中的最左边元素）和列上（维度1，即形状中左起第二个元素）连结两个矩阵。可以看到，输出的第一个`Tensor`在维度0的长度（$6$）为两个输入矩阵在维度0的长度之和（$3+3$），而输出的第二个`Tensor`在维度1的长度（$8$）为两个输入矩阵在维度1的长度之和（$4+4$）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T01:43:50.394668Z",
     "start_time": "2019-10-24T01:43:50.347759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [ 2,  1,  4,  3],\n",
      "        [ 1,  2,  3,  4],\n",
      "        [ 4,  3,  2,  1]])\n",
      "tensor([[ 0,  1,  2,  3,  2,  1,  4,  3],\n",
      "        [ 4,  5,  6,  7,  1,  2,  3,  4],\n",
      "        [ 8,  9, 10, 11,  4,  3,  2,  1]])\n"
     ]
    }
   ],
   "source": [
    "print( torch.cat((X, Y),0) )\n",
    "print( torch.cat((X, Y),1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用条件判断式可以得到元素为0或1的新的`Tensor`。以`X == Y`为例，如果`X`和`Y`在相同位置的条件判断为真（值相等），那么新的`Tensor`在相同位置的值为1；反之为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T08:41:04.497747Z",
     "start_time": "2019-11-19T08:41:04.466505Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X == Y).to(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对`Tensor`中的所有元素求和得到只有一个元素的`Tensor`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T08:41:31.863238Z",
     "start_time": "2019-11-19T08:41:31.816366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过[`item()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.item)函数将结果变换为Python中的标量。下面例子中`X`的$L_2$范数结果同上例一样是单元素`Tensor`，但最后结果变换成了Python中的标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T02:00:11.819840Z",
     "start_time": "2019-10-24T02:00:11.785338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.494443758403985"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.to(torch.float64).norm().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ㊟ [`torch.norm()`](https://pytorch.org/docs/stable/torch.html#torch.norm)函数的作用是求`tensor`的范数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以把`Y.to(torch.float64).exp()`、`X.sum()`、`X.to(torch.float64).norm()`等分别改写为`torch.exp(Y.to(torch.float64))`、`torch.sum(X)`、`torch.norm(X.to(torch.float64))`等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3 广播机制\n",
    "\n",
    "前面我们看到如何对两个形状相同的`Tensor`做按元素运算。当对两个形状不同的`Tensor`按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个`Tensor`形状相同后再按元素运算。\n",
    "\n",
    "定义两个`Tensor`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T08:44:39.111182Z",
     "start_time": "2019-11-19T08:44:39.055886Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]), tensor([[0, 1]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(3).reshape((3, 1))\n",
    "B = torch.arange(2).reshape((1, 2))\n",
    "A, B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于`A`和`B`分别是3行1列和1行2列的矩阵，如果要计算`A + B`，那么`A`中第一列的3个元素被广播（复制）到了第二列，而`B`中第一行的2个元素被广播（复制）到了第二行和第三行。如此，就可以对2个3行2列的矩阵按元素相加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T08:44:47.449448Z",
     "start_time": "2019-11-19T08:44:47.433822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.4 索引\n",
    "\n",
    "在`Tensor`中，索引（index）代表了元素的位置。`Tensor`的索引从0开始逐一递增。例如，一个3行2列的矩阵的行索引分别为0、1和2，列索引分别为0和1。\n",
    "\n",
    "在下面的例子中，我们指定了`Tensor`的行索引截取范围`[1:3]`。依据左闭右开指定范围的惯例，它截取了矩阵`X`中行索引为1和2的两行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T08:47:31.114896Z",
     "start_time": "2019-11-19T08:47:31.081471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X)\n",
    "X[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以指定`Tensor`中需要访问的单个元素的位置，如矩阵中行和列的索引，并为该元素重新赋值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T08:48:02.893027Z",
     "start_time": "2019-11-19T08:48:02.856551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  9,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1, 2] = 9\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，我们也可以截取一部分元素，并为它们重新赋值。在下面的例子中，我们为行索引为1的每一列元素重新赋值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T02:09:05.141833Z",
     "start_time": "2019-10-24T02:09:05.110589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [12, 12, 12, 12],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1:2, :] = 12\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.5 运算的内存开销\n",
    "\n",
    "在前面的例子里我们对每个操作新开内存来存储运算结果。举个例子，即使像`Y = X + Y`这样的运算，我们也会新开内存，然后将`Y`指向新内存。为了演示这一点，我们可以使用Python自带的`id`函数：如果两个实例的ID一致，那么它们所对应的内存地址相同；反之则不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T02:10:17.266638Z",
     "start_time": "2019-10-24T02:10:17.241755Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想指定结果到特定内存，我们可以使用前面介绍的索引来进行替换操作。在下面的例子中，我们先通过`zeros_like`创建和`Y`形状相同且元素为0的`Tensor`，记为`Z`。接下来，我们把`X + Y`的结果通过`[:]`写进`Z`对应的内存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T08:49:42.660667Z",
     "start_time": "2019-11-19T08:49:42.629418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = torch.zeros(Y.shape)\n",
    "before = id(Z)\n",
    "Z[:] = X + Y\n",
    "id(Z) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上，上例中我们还是为`X + Y`开了临时内存来存储计算结果，再复制到`Z`对应的内存。如果想避免这个临时内存开销，我们可以使用运算符全名函数中的`out`参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T02:19:31.532205Z",
     "start_time": "2019-10-24T02:19:31.500955Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(X, Y, out=Z)\n",
    "id(Z) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果`X`的值在之后的程序中不会复用，我们也可以用 `X[:] = X + Y` 或者 `X += Y` 来减少运算的内存开销。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T02:20:06.940455Z",
     "start_time": "2019-10-24T02:20:06.913939Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.6 `Tensor`和NumPy相互变换\n",
    "\n",
    "我们可以通过[`tensor()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)函数和[`numpy()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.numpy)函数令数据在`Tensor`和NumPy格式之间相互变换。下面将NumPy实例变换成`Tensor`实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T08:50:01.467927Z",
     "start_time": "2019-11-19T08:50:01.374200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "P = np.ones((2, 3))\n",
    "D = torch.tensor(P)\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再将`Tensor`实例变换成NumPy实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T02:31:07.870084Z",
     "start_time": "2019-10-24T02:31:07.791969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* `Tensor`是PyTorch中存储和变换数据的主要工具。\n",
    "* 可以轻松地对`Tensor`创建、运算、指定索引，并与NumPy之间相互变换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习\n",
    "\n",
    "* 运行本节中的代码。将本节中条件判断式`X == Y`改为`X < Y`或`X > Y`，看看能够得到什么样的`Tensor`。\n",
    "* 将广播机制中按元素运算的两个`Tensor`替换成其他形状，结果是否和预期一样？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 扫码直达[知乎专栏](https://zhuanlan.zhihu.com/unicom-d2l)\n",
    "\n",
    "![](../img/zhihu.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
